# VSM Phoenix Environment Variables Example

# Database Configuration
DATABASE_URL=ecto://USER:PASS@HOST/DATABASE

# Phoenix Configuration
SECRET_KEY_BASE=your-secret-key-base
PHX_HOST=localhost
PORT=4000

# VSM System Configuration
VSM_QUEEN_CHECK_INTERVAL=30000
VSM_VIABILITY_THRESHOLD=0.8
VSM_INTERVENTION_THRESHOLD=0.7

VSM_INTELLIGENCE_SCAN_INTERVAL=300000
VSM_ADAPTATION_TIMEOUT=600000
VSM_LEARNING_RATE=0.05

VSM_CONTROL_OPTIMIZATION_INTERVAL=60000
VSM_COMPUTE_THRESHOLD=0.75
VSM_MEMORY_THRESHOLD=0.8
VSM_NETWORK_THRESHOLD=0.65
VSM_STORAGE_THRESHOLD=0.85

VSM_COORDINATOR_SYNC_INTERVAL=15000
VSM_OSCILLATION_WINDOW=10000
VSM_MAX_MESSAGE_FREQ=50

VSM_OPERATIONS_HEALTH_INTERVAL=45000
VSM_MAX_PROCESSING_TIME=500
VSM_CUSTOMER_RESPONSE_TARGET=200

# Telegram Bot Configuration
# Get your bot token from @BotFather on Telegram
TELEGRAM_BOT_TOKEN=your-bot-token-here

# Webhook mode (true/false) - Use polling by default
TELEGRAM_WEBHOOK_MODE=false

# Webhook URL (required if webhook mode is true)
# Example: https://yourdomain.com/telegram/webhook
TELEGRAM_WEBHOOK_URL=

# Comma-separated list of authorized chat IDs
# Example: 123456789,987654321
TELEGRAM_AUTHORIZED_CHATS=

# Comma-separated list of admin chat IDs (subset of authorized chats)
# Example: 123456789
TELEGRAM_ADMIN_CHATS=

# Rate limit (messages per minute per chat)
TELEGRAM_RATE_LIMIT=30

# Command timeout in milliseconds
TELEGRAM_COMMAND_TIMEOUT=5000

# Tidewave Integration (optional)
TIDEWAVE_ENABLED=false
TIDEWAVE_ENDPOINT=https://api.tidewave.io
TIDEWAVE_API_KEY=your-tidewave-api-key
TIDEWAVE_TIMEOUT=30000

# AMQP Configuration (for RabbitMQ)
AMQP_URL=amqp://guest:guest@localhost:5672

# LLM API Configuration
# OpenAI API Key (get from https://platform.openai.com/api-keys)
OPENAI_API_KEY=your-openai-api-key

# Anthropic API Key (get from https://console.anthropic.com/account/keys)
ANTHROPIC_API_KEY=your-anthropic-api-key

# LLM Configuration
# Enable LLM variety analysis (true/false)
ENABLE_LLM_VARIETY=false

# LLM Rate Limits (requests per minute)
LLM_RATE_LIMIT=60

# LLM Cache TTL (in hours)
LLM_CACHE_TTL=24

# Default LLM Provider (openai or anthropic)
DEFAULT_LLM_PROVIDER=openai

# Default Models
OPENAI_DEFAULT_MODEL=gpt-4-turbo
ANTHROPIC_DEFAULT_MODEL=claude-3-sonnet