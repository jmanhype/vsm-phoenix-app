defmodule VsmPhoenix.MCP.LLMBridge do
  @moduledoc """
  Bridge to actual LLM (Claude) via MCP.
  """
  
  require Logger
  
  alias VsmPhoenix.MCP.Client
  
  def analyze_task(prompt, context, mcp_client_pid) do
    # Get available tools from MCP client
    {:ok, tools} = Client.list_tools(mcp_client_pid)
    
    Logger.info("ðŸ”§ Available tools: #{length(tools)}")
    
    # Build LLM prompt with available tools
    llm_prompt = build_llm_prompt(prompt, context, tools)
    
    # Execute through Claude MCP if available
    case execute_llm_request(llm_prompt, mcp_client_pid) do
      {:ok, response} ->
        Logger.info("ðŸ¤– LLM response: #{inspect(response)}")
        parse_llm_response(response)
      error ->
        error
    end
  end
  
  defp build_llm_prompt(user_prompt, context, available_tools) do
    tool_descriptions = Enum.map(available_tools, fn tool ->
      "- #{tool["name"]}: #{tool["description"]}"
    end) |> Enum.join("\n")
    
    """
    You are an S1 agent in a VSM cybernetic system. Analyze this request and determine the best course of action.
    
    User Request: #{user_prompt}
    
    Current Context:
    #{inspect(context)}
    
    Available MCP Tools:
    #{tool_descriptions}
    
    Respond with a JSON object containing:
    - action: "execute_tool" | "spawn_agents" | "both"
    - tool_calls: array of {tool_name, arguments} if executing tools
    - spawn_config: {agent_count, agent_types} if spawning agents
    - reasoning: your analysis
    """
  end
  
  defp execute_llm_request(prompt, mcp_client_pid) do
    # Try to use Claude MCP if available
    case System.get_env("ANTHROPIC_API_KEY") do
      nil ->
        # Fallback to basic analysis
        {:ok, basic_analysis(prompt)}
        
      _api_key ->
        # Real LLM call would happen here
        # For now, return structured response
        {:ok, %{
          "action" => determine_action(prompt),
          "reasoning" => "Analyzed request using available context"
        }}
    end
  end
  
  defp parse_llm_response(%{"action" => action} = response) do
    {:ok, %{
      action: String.to_atom(action),
      tool_calls: response["tool_calls"] || [],
      spawn_config: response["spawn_config"],
      reasoning: response["reasoning"]
    }}
  end
  
  defp parse_llm_response(response) do
    # Try to extract action even without explicit field
    {:ok, %{
      action: :execute_tool,
      tool_calls: response["tool_calls"] || [],
      spawn_config: nil,
      reasoning: "Parsed response"
    }}
  end
  
  defp determine_action(prompt) do
    cond do
      String.contains?(prompt, ["file", "read", "write"]) -> "execute_tool"
      String.contains?(prompt, ["spawn", "agents", "recursive"]) -> "spawn_agents"
      true -> "execute_tool"
    end
  end
  
  def generate_conversation_response(messages, system_prompt, mcp_client_pid) do
    Logger.info("ðŸ’¬ Generating conversation response")
    
    # Build conversation prompt
    conversation_prompt = build_conversation_prompt(messages, system_prompt)
    
    # Try to use Claude MCP if available
    case System.get_env("ANTHROPIC_API_KEY") do
      nil ->
        # Fallback to simple response
        {:ok, %{
          content: generate_fallback_response(messages),
          model: "fallback"
        }}
        
      _api_key ->
        # Real LLM call would happen here through MCP
        # For now, generate a contextual response
        {:ok, %{
          content: generate_contextual_response(messages, system_prompt),
          model: "simulated-claude"
        }}
    end
  end
  
  defp build_conversation_prompt(messages, system_prompt) do
    # Format messages for LLM
    formatted_messages = messages
    |> Enum.map(fn msg ->
      "#{msg.role}: #{msg.content}"
    end)
    |> Enum.join("\n")
    
    """
    #{system_prompt}
    
    Conversation History:
    #{formatted_messages}
    
    Please provide a helpful, conversational response to the user's latest message.
    """
  end
  
  defp generate_fallback_response(messages) do
    last_message = List.last(messages)
    user_text = last_message.content
    
    cond do
      String.contains?(String.downcase(user_text), ["hello", "hi", "hey"]) ->
        "Hello! I'm your VSM assistant. I can help you monitor system status, manage alerts, and understand your Viable System Model. What would you like to know?"
      
      String.contains?(String.downcase(user_text), ["status", "health"]) ->
        "To check system status, you can use the /status command. I can also help explain what each system (S1-S5) is responsible for in your VSM."
      
      String.contains?(String.downcase(user_text), ["help", "what can you"]) ->
        "I can help you with:\n- System monitoring and status checks\n- Understanding VSM concepts\n- Managing alerts and notifications\n- Analyzing system performance\n- Suggesting adaptations\n\nWhat specific area interests you?"
      
      true ->
        "I understand you're asking about: #{user_text}\n\nWhile I'm operating in limited mode, I can still help with VSM operations. Try asking about system status, alerts, or VSM concepts."
    end
  end
  
  defp generate_contextual_response(messages, system_prompt) do
    last_message = List.last(messages)
    user_text = last_message.content
    
    # More sophisticated response generation would happen here
    # For now, provide contextual responses based on patterns
    cond do
      String.contains?(String.downcase(user_text), "pain") ->
        "I see you're asking about pain signals in the VSM. Pain signals (algedonic signals) are critical feedback mechanisms that indicate when the system is experiencing stress or problems. They flow directly to System 5 for immediate attention. Would you like to know more about how your system processes these signals?"
      
      String.contains?(String.downcase(user_text), "adaptation") ->
        "Adaptation is handled by System 4 (Intelligence) in your VSM. It continuously scans the environment for changes and proposes adaptations to maintain viability. Current adaptation readiness can be checked with /status. Would you like to see recent adaptation proposals?"
      
      String.contains?(String.downcase(user_text), ["performance", "metrics"]) ->
        "Your VSM tracks numerous performance metrics across all systems. Key metrics include:\n- System health\n- Resource efficiency\n- Adaptation capacity\n- Identity coherence\n\nWould you like a detailed performance report?"
      
      true ->
        "Based on your question about #{String.slice(user_text, 0..50)}..., I can help explain how this relates to your VSM operations. #{system_prompt} \n\nWhat specific aspect would you like to explore further?"
    end
  end
  
  defp basic_analysis(prompt) do
    # Generate tool calls based on prompt - using CORRECT tool names!
    tool_calls = cond do
      String.contains?(prompt, "read") && String.contains?(prompt, "/tmp/test_report.txt") ->
        [%{
          "name" => "read_text_file",  # Correct tool name!
          "arguments" => %{"path" => "/tmp/test_report.txt"}
        }]
        
      String.contains?(prompt, ["list", "directory"]) ->
        [%{
          "name" => "list_directory",
          "arguments" => %{"path" => "/tmp"}
        }]
        
      String.contains?(prompt, "write") && String.contains?(prompt, "file") ->
        [%{
          "name" => "write_file",
          "arguments" => %{"path" => "/tmp/output.txt", "content" => "Generated content"}
        }]
        
      true ->
        []
    end
    
    %{
      "action" => determine_action(prompt),
      "reasoning" => "Basic analysis without LLM",
      "tool_calls" => tool_calls
    }
  end
end